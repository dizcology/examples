{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7H3yTncQfoym"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "h1CiDh7CfqON"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty Detection with Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1sepk9uMddm"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdl5_0Z4-w41"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, intermediate_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
    "    \n",
    "    def call(self, input_features):\n",
    "        activation = self.hidden_layer(input_features)\n",
    "        return self.output_layer(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3za66lwMjWX"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu)\n",
    "  \n",
    "    def call(self, code):\n",
    "        activation = self.hidden_layer(code)\n",
    "        return self.output_layer(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rl5HUez7-w5C"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, intermediate_dim, original_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.loss = []\n",
    "    self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
    "    self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "\n",
    "  def call(self, input_features):\n",
    "    code = self.encoder(input_features)\n",
    "    reconstructed = self.decoder(code)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmWZG-1qLP8p"
   },
   "outputs": [],
   "source": [
    "def loss(preds, real):\n",
    "  return tf.reduce_mean(tf.square(tf.subtract(preds, real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNSJqjY7qnCe"
   },
   "outputs": [],
   "source": [
    "def train(loss, model, opt, original):\n",
    "  with tf.GradientTape() as tape:\n",
    "    preds = model(original)\n",
    "    reconstruction_error = loss(preds, original)\n",
    "  gradients = tape.gradient(reconstruction_error, model.trainable_variables)\n",
    "  gradient_variables = zip(gradients, model.trainable_variables)\n",
    "  opt.apply_gradients(gradient_variables)\n",
    "  \n",
    "  return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8Sh1UaQrc5D"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss, dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for step, batch_features in enumerate(dataset):\n",
    "      loss_values = train(loss, model, opt, batch_features)\n",
    "      epoch_loss += loss_values\n",
    "    model.loss.append(epoch_loss)\n",
    "    print('Epoch {}/{}. Loss: {}'.format(epoch + 1, epochs, epoch_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eCxbz9ZSwjr"
   },
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the novelty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes several minutes to download and prepare\n",
    "\n",
    "emnist_ds = tfds.load('emnist/letters:3.*.*', split='test')\n",
    "\n",
    "def transform(d):\n",
    "    image = d['image']\n",
    "    label = d['label']\n",
    "    \n",
    "    image = tf.transpose(image, perm=(1, 0, 2))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return {'image': image, 'label': label}\n",
    "\n",
    "emnist_ds = emnist_ds.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_ds = tfds.load('fashion_mnist:3.*.*', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_ds = emnist_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_data = np.array([x['image'].numpy() for x in novelty_ds]).reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_data.shape, novelty_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, x_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "268qdJGGTULP"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "intermediate_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8nw7mdKMxvb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)\n",
    "\n",
    "model = Autoencoder(intermediate_dim=intermediate_dim, original_dim=784)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "train_loop(model, opt, loss, training_dataset, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VioflTOhTnwl"
   },
   "source": [
    "## Plot the in-training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azgmhikhM0EE"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), model.loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JccKWvNYTtUW"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(indices, data=x_test):\n",
    "    number = len(indices)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, index in enumerate(indices):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, number, i + 1)\n",
    "        original = data[index].reshape(28, 28)\n",
    "        reconstructed = model(data)[index].numpy().reshape(28, 28)\n",
    "\n",
    "        # the displayed error is scaled\n",
    "        error = np.round(np.square(original - reconstructed).sum(), 3)\n",
    "\n",
    "        plt.imshow(original * 255)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, number, i + 1 + number)\n",
    "        plt.imshow(reconstructed * 255)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.text(0, -1, error, fontdict={'size': 25})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(range(len(y_test)), 10)\n",
    "display(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(range(len(novelty_data)), 10)\n",
    "display(indices, data=novelty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort novelty data by reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originals = novelty_data\n",
    "reconstructeds = model(novelty_data).numpy()\n",
    "\n",
    "errors = np.square(originals - reconstructeds).sum(axis=-1).astype(int)\n",
    "\n",
    "sorted_args = np.argsort(errors)\n",
    "in_indices = sorted_args[:10]\n",
    "out_indices = sorted_args[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(in_indices, novelty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(out_indices, novelty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2000\n",
    "display(sorted_args[start:start+10], novelty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reconstruction errors from the two test data sets\n",
    "test_reconstructed = model(x_test).numpy()\n",
    "test_errors = np.round(np.square(x_test - test_reconstructed).sum(-1), 3)\n",
    "\n",
    "novelty_reconstructed = model(novelty_data).numpy()\n",
    "novelty_errors = np.round(np.square(novelty_data - novelty_reconstructed).sum(-1), 3)\n",
    "\n",
    "print(test_errors.shape, novelty_errors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(novelty_errors, bins=30, fc=(0, 0, 1, 0.5))\n",
    "plt.hist(test_errors, bins=30, fc=(1, 0, 0, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
